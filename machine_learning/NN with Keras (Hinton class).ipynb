{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T03:11:06.174456Z",
     "start_time": "2017-06-09T03:11:06.149641Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, Imputer\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Activation, Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T03:10:02.499823Z",
     "start_time": "2017-06-09T03:10:02.481908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.figsize\": (12, 9)})\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T02:52:40.976430Z",
     "start_time": "2017-06-09T02:52:40.083201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras.datasets import mnist\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T02:52:44.095411Z",
     "start_time": "2017-06-09T02:52:43.900319Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels \n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels \n",
    "X_val = mnist.validation.images\n",
    "y_val = mnist.validation.labels\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "X_val = X_val.reshape(X_val.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T03:25:00.127945Z",
     "start_time": "2017-06-09T03:25:00.120680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001, 8.333333333333333e-05)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 12\n",
    "# layers = [784, 256, 128]\n",
    "learning_rate = 0.001\n",
    "decay_rate = learning_rate / epochs\n",
    "learning_rate, decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T03:25:00.584832Z",
     "start_time": "2017-06-09T03:25:00.580651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550000, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T03:25:01.180255Z",
     "start_time": "2017-06-09T03:25:01.174788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_dim = X_train.shape[1]\n",
    "# output_dim = y_train.shape[1]\n",
    "num_classes = 10\n",
    "# print(X_train.shape, X_val.shape, X_test.shape)\n",
    "# print(input_dim, output_dim)\n",
    "n_samples, input_dim, img_rows, img_cols = X_train.shape\n",
    "n_samples, input_dim, img_rows, img_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T03:25:03.136147Z",
     "start_time": "2017-06-09T03:25:03.102325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 1, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 43120000 into shape (55000,28,1,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-e7f2d5295dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 43120000 into shape (55000,28,1,1)"
     ]
    }
   ],
   "source": [
    "input_shape = (img_rows, img_cols, 1)\n",
    "print(input_shape)\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "y_val = np_utils.to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T03:25:06.081756Z",
     "start_time": "2017-06-09T03:25:05.997557Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_17/convolution' (op: 'Conv2D') with input shapes: [?,28,1,1], [3,3,1,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_17/convolution' (op: 'Conv2D') with input shapes: [?,28,1,1], [3,3,1,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f62335624eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m model.add(Conv2D(32, kernel_size=(3, 3),\n\u001b[1;32m     21\u001b[0m                  \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                  input_shape=input_shape))\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3093\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3094\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3095\u001b[0;31m         data_format='NHWC')\n\u001b[0m\u001b[1;32m   3096\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         op=op)\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[0;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dilation_rate must be positive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(input_converted, _, padding)\u001b[0m\n\u001b[1;32m    651\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     return with_space_to_batch(\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[0;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[1;32m    127\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NDHWC\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    404\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_17/convolution' (op: 'Conv2D') with input shapes: [?,28,1,1], [3,3,1,32]."
     ]
    }
   ],
   "source": [
    "# input layer\n",
    "# inputs = Input(shape=(input_dim,), dtype='float32', name='main_input')\n",
    "# with_dropout = False\n",
    "\n",
    "# hidden layers\n",
    "# x = Dense(256, activation='relu')(inputs)\n",
    "# # x = Dropout(0.1)(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# x = Dense(56, activation='relu')(x)\n",
    "# # x = Dense(28, activation='relu')(x)\n",
    "\n",
    "# img_rows, img_cols = 28, 28\n",
    "\n",
    "\n",
    "# input_shape = (1, img_rows, img_cols)\n",
    "# num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "# print(num_pixels)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# output layer\n",
    "# predictions = Dense(output_dim, activation='softmax', name='main_output')(x)\n",
    "# model.add(Dense(output_dim, activation='softmax', name='main_output'))\n",
    "# opt = Adam(lr=learning_rate) #, decay=decay_rate)\n",
    "# momentum = 0.8\n",
    "# opt = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=True)\n",
    "# model = Model(inputs=inputs, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T02:38:04.085868Z",
     "start_time": "2017-06-09T02:38:04.068274Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have 4 dimensions, but got array with shape (55000, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b098cc8ef379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m hist_model = model.fit(X_train, y_train, epochs=epochs, batch_size=200, verbose=2, \n\u001b[0;32m----> 2\u001b[0;31m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                       )\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1430\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1307\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1310\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1311\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have 4 dimensions, but got array with shape (55000, 10)"
     ]
    }
   ],
   "source": [
    "hist_model = model.fit(X_train, y_train, epochs=epochs, batch_size=200, verbose=2, \n",
    "                       validation_data=(X_val, y_val)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy = 0.9811\n",
      "Loss = 0.1175\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"\\nAccuracy = {:.4f}\".format(accuracy))\n",
    "print(\"Loss = {:.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAFkCAYAAAAjVP3NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146a04e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(hist_model.history['acc']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
