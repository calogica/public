{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T02:09:18.797274Z",
     "start_time": "2017-05-31T02:09:18.649172Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "np.random.seed(42)\n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:41:34.060455Z",
     "start_time": "2017-05-31T01:41:28.041528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:47:50.078930Z",
     "start_time": "2017-05-31T01:47:50.074977Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels \n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T01:48:16.649014Z",
     "start_time": "2017-05-31T01:48:16.643520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (55000, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T03:31:57.870570Z",
     "start_time": "2017-05-31T03:31:57.867408Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T14:19:49.723301Z",
     "start_time": "2017-05-31T14:19:49.354694Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionTF(object):\n",
    "\n",
    "    def __init__(self, eta=0.01, epochs=50, display_step = 10, batch_size=10, regularization=0.01,\n",
    "                hidden_layers=[256,128]):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.display_step_ = display_step\n",
    "        self.batch_size_ = batch_size\n",
    "        self.hidden_layers_ = hidden_layers\n",
    "        self.regularization_ = regularization\n",
    "        \n",
    "    def get_minibatches(self, X):\n",
    "        return zip(range(0, len(X), self.batch_size_), \n",
    "                                      range(self.batch_size_, \n",
    "                                      len(X)+1, self.batch_size_))\n",
    "        \n",
    "    def train(self, X_train, y_train, X_test, y_test):\n",
    "        \n",
    "        n_samples, n_features = X_train.shape\n",
    "        _, n_classes = y_train.shape\n",
    "        self.cost_ = []\n",
    "    \n",
    "        X_ = tf.placeholder(tf.float32, name='features')\n",
    "        Y_ = tf.placeholder(tf.float32, name='targets')\n",
    "        ri = tf.random_normal_initializer()\n",
    "        ci = tf.constant_initializer(0)\n",
    "        \n",
    "\n",
    "        weights = {}\n",
    "        biases = {}\n",
    "        num_layers = len(self.hidden_layers_)\n",
    "        for layer, layer_size in enumerate(self.hidden_layers_):\n",
    "            n = layer_size #if layer == 0 else self.hidden_layers[layer-1]\n",
    "            m = self.hidden_layers_[layer+1] if layer+1 < num_layers else n_classes\n",
    "\n",
    "            w_name = 'w{:}'.format(layer)\n",
    "            b_name = 'b{:}'.format(layer)\n",
    "            weights[str(layer)] = tf.Variable(tf.truncated_normal([n, m], stddev=0.1), name=w_name)\n",
    "            biases[str(layer)] = tf.Variable(tf.zeros([m]), name=b_name)\n",
    "            \n",
    "        # Model\n",
    "        layers = {}\n",
    "        layers['0'] = tf.nn.relu(tf.matmul(X_, weights['0']) +  biases['0'])\n",
    "        \n",
    "        for i in range(1, num_layers):\n",
    "            \n",
    "            z = tf.matmul(layers[str(i-1)], weights[str(i)]) + biases[str(i)]\n",
    "            activation = tf.nn.relu(z)\n",
    "            layers[str(i)] = activation\n",
    "                    \n",
    "        out_z = tf.matmul(layers[str(num_layers-2)], \n",
    "                                 weights[str(num_layers-1)]) + biases[str(num_layers-1)]   \n",
    "        \n",
    "        out_act = tf.nn.sigmoid(out_z, name='predicted_probabilities')\n",
    "        out_labels = tf.argmax(out_z, axis=1, name='predicted_labels')\n",
    "    \n",
    "        l2_loss = 0.\n",
    "        for w in weights.values():\n",
    "            l2_loss += tf.nn.l2_loss(w)\n",
    "            \n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=out_z, labels=Y_)\n",
    "        cost = tf.reduce_mean(loss, name='cost') + self.regularization_ * (l2_loss)\n",
    "\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.eta)\n",
    "        train_step = optimizer.minimize(cost, name='train')\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(Y_, 1), out_labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    \n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            for i in range(self.epochs):\n",
    "                \n",
    "                avg_cost = 0.\n",
    "                \n",
    "                x, y = shuffle(X_train, y_train)\n",
    "                \n",
    "                for start, end in self.get_minibatches(x):\n",
    "                    _, c = sess.run(['train', 'cost:0'], feed_dict={'features:0': x[start:end],\n",
    "                                                            'targets:0': y[start:end]})\n",
    "                    \n",
    "                    avg_cost += c\n",
    "\n",
    "                if (i+1) % self.display_step_ == 0:\n",
    "\n",
    "                    train_acc = sess.run('accuracy:0', feed_dict={'features:0': X_train,\n",
    "                                                          'targets:0': y_train})\n",
    "                    valid_acc = sess.run('accuracy:0', feed_dict={'features:0': X_test,\n",
    "                                                          'targets:0': y_test})  \n",
    "\n",
    "                    print(\"Epoch: %03d | AvgCost: %.3f\" % (i + 1, avg_cost / (i + 1)), end=\"\")\n",
    "                    print(\" | Train/Valid ACC: %.3f/%.3f\" % (train_acc, valid_acc))\n",
    "\n",
    "        return self\n",
    "    \n",
    "#     def predict(self, X):        \n",
    "#         return self.W * X + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-31T14:32:23.598627Z",
     "start_time": "2017-05-31T14:29:32.563337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010 | AvgCost: 9.988 | Train/Valid ACC: 0.944/0.943\n",
      "Epoch: 020 | AvgCost: 4.540 | Train/Valid ACC: 0.951/0.952\n",
      "Epoch: 030 | AvgCost: 2.924 | Train/Valid ACC: 0.952/0.952\n",
      "Epoch: 040 | AvgCost: 2.162 | Train/Valid ACC: 0.953/0.953\n",
      "Epoch: 050 | AvgCost: 1.716 | Train/Valid ACC: 0.953/0.953\n",
      "Epoch: 060 | AvgCost: 1.422 | Train/Valid ACC: 0.952/0.951\n",
      "Epoch: 070 | AvgCost: 1.214 | Train/Valid ACC: 0.955/0.955\n",
      "Epoch: 080 | AvgCost: 1.057 | Train/Valid ACC: 0.954/0.954\n",
      "Epoch: 090 | AvgCost: 0.940 | Train/Valid ACC: 0.954/0.954\n",
      "Epoch: 100 | AvgCost: 0.842 | Train/Valid ACC: 0.954/0.953\n"
     ]
    }
   ],
   "source": [
    "n_iter = 100\n",
    "eta = 0.01\n",
    "hidden_layers = [784, 256, 128]\n",
    "\n",
    "lr_tf = LogisticRegressionTF(eta, n_iter, display_step=10, batch_size=128, hidden_layers=hidden_layers)\n",
    "lr_tf = lr_tf.train(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
